digraph
{
	line;
	saved_line;
	token;
	source_file;
	error_flag;
	in_comment;
	data_base;
	free;
	saved_list;

	// noise is not used

	// open should be inlined
	// is_console should be inlined
	// strip_leading_blanks should be slain
	// strip_trailing_blanks should be slain
	// toupper isn't worth it; just make EXIT and exit work, don't worry about ExIt
	// is_number should be removed
	// head is from the lisp runtime
	// tail is from the lisp runtime
	// allocation_size is irrelevant
	// node_size is irrelevant
	// normalize is irrelevant
	// string_val is an accessor for node
	// tag_value is an accessor for node
	// print_list is unused, just for debugging
	// get_memory is part of the freelist management
	// get_from_free is get_memory's helper
	// alloc_str is part node constructor, and part memory management
	// cons is from the lisp runtime.
	// append_list is straight out of lisp.
	// list_length is straight out of lisp.
	// collect_garbage is nearly irrelevant.
	// lower is irrelevant
	// mark, unmark, release_mem are nearly irrelevant.
	// free_memory is part of the freelist management
	// do_release is nearly irrelevant
	// test_memory is nearly irrelevant
	// wait is support for the interactive repl
	// read_kbd is irrelevant - we can share code interactive / noninteractive reading
	// read_from_file handles comments, and manages line and saved_line
	// read_a_line handles the eof_mark
	// get_token isn't doing anything terribly tricky, just part of the lexer
	// get_word is part of get_token
	// comment is part of get_token (but wasn't that handled already by read_from_file?)
	// get_quote is part of get_token (do we really have to support quoted strings?)
	// scan is the outer interface to the lexer,
	// compile
	// error uses saved_line and line to point a caret to where the error occurred
	// runout could be inlined, it's not doing anything tricky.
	// goal is a nonterminal in the grammar
	//   there are three possibilities - a quoted string, a functor, or a constant.
	// functor a nonterminal in the grammar, part of goal.
	//   functor assumes the "foo(" has been parsed, and concentrates on the arguments and the end paren
	//   functor allocates a "c_ptr" on the stack, and passes it into components by reference
	// components is a nonterminal in the grammar, part of functor.
	//   components parses a sequence of terms, separated by commas
	// term is a nonterminal in the grammar, part of components
	//   term inspects the first characater and delegates to some smaller things (varbl, quoted_str, number)
	//   or tosses it back up to scan and functor
	// quoted_str is a nonterminal in the grammar, part of term
	// varbl is a nonterminal in the grammar, part of term (Don't tolerate abbreviations or misspellings!)
	// number should be removed - they're effectively just constants
	// tail_list is part of the recursive descent compiler.
	//   captures a nonempty sequence of goals separated by commas.
	// rule is part of the recursive descent compiler.
	//   rule parses facts and rules (that is, whether or not they have tails).
	//   at the end of rule parsing, it appends to the database global
	// head_list is part of the recursive descent compiler, and just delegates to goal
	// query
	// look_up is straightforwardly walking a variable through a substitution list
	// check_continue is part of the REPL - checking whether the user wants another solutioon
	// print_bindings is printing answers
	// print_functor, print_variable, print_components are all part of print_bindings
	// copy_list copies a list  ***and appends the copy level to all variables***
	// list_copy is copy_list's helper
	// make_binding is a standard environment manipulation, I think
	// unify makes (primarily) its own return value and new_environ lexically available
	// fail communicates back to unify using lexical variables
	// unify_constant  unify_func, unify_tail, unify_expr are all part of the double-dispatch architecture of unify
	// solve makes sure that things won't be garbage collected,
 	//   then walks through the database, trying to find a rule that will unify
	//   if solve finds a rule that matches, it appends the tail of the rule and recurses
	//   solve creates new_env on the stack, and passes it by reference into unify, which makes it available lexically
	// query is part of the recursive descent parser
	//   after it parses (using tail_list) it calls solve
	// read_new_file copies globals into locals, delegates (recurses) to compile),
	//   then puts the globals back
	// do_exit handles the EXIT command
	// compile uses the first token to decide what action to do - a query, include, exit, or rule.
	// initialize sets up globals
	// main calls initialize and then compile, starting compile on stdin
}
